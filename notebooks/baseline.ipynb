{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a87c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199e1c81",
   "metadata": {},
   "source": [
    "## Настройка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d5731-7f24-428f-9ef9-4f3608e63079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/OPR-Project/OpenPlaceRecognition.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5eeec-5a2c-4e1c-bb62-fb107b3abb37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c0792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "import torchshow as ts\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import opr\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from opr.models.place_recognition import BoQModel, SequenceLateFusionModel\n",
    "from opr.modules.temporal import TemporalAveragePooling\n",
    "from opr.pipelines.place_recognition.sequential import SequencePlaceRecognitionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895fdc9-defa-4aa1-9742-8e9dc1bbbd5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}, cuda: {torch.cuda.is_available()}\")\n",
    "print(f\"OpenPlaceRecognition version: {opr.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"FAISS version: {faiss.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c6ab7f",
   "metadata": {},
   "source": [
    "## Загрузка данных из S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e8bc1",
   "metadata": {},
   "source": [
    "Используйте предоставленный скрипт: `python ../scripts/download_data.py --help` (рекомендуется запускать в отдельном терминале, а не в ноутбуке)\n",
    "\n",
    "Или, например, перенесите код из скрипта и адаптируйте его под себя, если необходимо."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd83d8b",
   "metadata": {},
   "source": [
    "## Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38409a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPO_ROOT = Path.cwd().parent\n",
    "print(f\"Repository root dir: {REPO_ROOT}\")\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "assert DATA_DIR.exists(), f\"Data directory {DATA_DIR} does not exist. Please run the download script.\"\n",
    "print(f\"Data dir: {DATA_DIR}\")\n",
    "\n",
    "SUBMISSIONS_DIR = REPO_ROOT / \"submissions\"\n",
    "SUBMISSIONS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"Submissions dir: {SUBMISSIONS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98426c",
   "metadata": {},
   "source": [
    "## Чтение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319ac67",
   "metadata": {},
   "source": [
    "Создадим DataReader для чтения данных с диска.\n",
    "\n",
    "Ключевые моменты:\n",
    "- `__getitem__` возвращает словарь с ключами:\n",
    "  - `pose`: координаты в формате `[x, y]`\n",
    "  - `image_front_cam`: изображение передней камеры (если указан аргумент `front_cam=True`)\n",
    "  - `image_back_cam`: изображение задней камеры (если указан аргумент `back_cam=True`)\n",
    "- `collate_fn` объединяет данные в батчи в нужном для OPR формате: словарь с ключами `poses`, `images_<camera_name>`, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59646bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ITLPTrackDataReader:\n",
    "    def __init__(self, root: Path, image_transform: A.Compose, front_cam: bool = True, back_cam: bool = False):\n",
    "        self._root = Path(root)\n",
    "        self._front_cam_dir = self._root / \"front_cam\"\n",
    "        self._back_cam_dir = self._root / \"back_cam\"\n",
    "\n",
    "        self._track_df = pd.read_csv(self._root / \"track.csv\")\n",
    "        self._image_transform = image_transform  # note that we use albumentations for image transformations\n",
    "        self._front_cam = front_cam\n",
    "        self._back_cam = back_cam\n",
    "        if not self._front_cam and not self._back_cam:\n",
    "            raise ValueError(\"At least one camera must be enabled: front_cam or back_cam.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._track_df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict[str, Tensor]:\n",
    "        pose = self._track_df[[\"tx\", \"ty\"]].iloc[idx].to_numpy()\n",
    "        front_cam_path = self._front_cam_dir / f\"{self._track_df['front_cam_ts'].iloc[idx]}.jpg\"\n",
    "        back_cam_path = self._back_cam_dir / f\"{self._track_df['back_cam_ts'].iloc[idx]}.jpg\"\n",
    "\n",
    "        out_dict = {\"pose\": Tensor(pose)}\n",
    "\n",
    "        if self._front_cam:\n",
    "            front_cam_image = cv2.cvtColor(cv2.imread(str(front_cam_path)), cv2.COLOR_BGR2RGB)\n",
    "            front_cam_image = self._image_transform(image=front_cam_image)[\"image\"]  #\n",
    "            out_dict[\"image_front_cam\"] = front_cam_image\n",
    "        if self._back_cam:\n",
    "            back_cam_image = cv2.cvtColor(cv2.imread(str(back_cam_path)), cv2.COLOR_BGR2RGB)\n",
    "            back_cam_image = self._image_transform(image=back_cam_image)[\"image\"]\n",
    "            out_dict[\"image_back_cam\"] = back_cam_image\n",
    "\n",
    "        return out_dict\n",
    "\n",
    "    def collate_fn(self, batch: list[dict[str, Tensor]]) -> dict[str, Tensor]:\n",
    "        collated_batch = {}\n",
    "        for key in batch[0].keys():\n",
    "            if key.startswith(\"image_\"):\n",
    "                collated_batch[\"images_\" + key[6:]] = torch.stack([item[key] for item in batch])\n",
    "            elif key == \"pose\":\n",
    "                collated_batch[\"poses\"] = torch.stack([item[key] for item in batch])\n",
    "        return collated_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5fc90",
   "metadata": {},
   "source": [
    "Для трансформов изображений используем библиотеку `albumentations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46b85f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_transforms(image_size: int = 322) -> A.Compose:  # 384 for ResNet50, 322 for DINOv2\n",
    "    \"\"\"Create image transformation pipeline.\"\"\"\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.CenterCrop(height=720, width=720),  # Crop to 720x720 for 1:1 aspect ratio\n",
    "            A.Resize(height=image_size, width=image_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            A.pytorch.ToTensorV2(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c52ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "database_reader = ITLPTrackDataReader(\n",
    "    root=DATA_DIR / \"test\" / \"07_2023-10-04-day\",\n",
    "    image_transform=setup_transforms(image_size=322)  # Use 384 for ResNet50, 322 for DINOv2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b74540",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_reader[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_show = 1\n",
    "ts.show(database_reader[id_to_show][\"image_front_cam\"])\n",
    "print(f\"Pose: {database_reader[id_to_show]['pose'].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fb63d",
   "metadata": {},
   "source": [
    "## Инициализация модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0dcd47",
   "metadata": {},
   "source": [
    "Сначала обычная модель для одиночных фреймов данных.\n",
    "\n",
    "Здесь мы используем Bag-of-Queries (BoQ) с бэкбоном DINOv2.\n",
    "О методе - https://github.com/amaralibey/Bag-of-Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BoQModel(backbone_name=\"dinov2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7affa4",
   "metadata": {},
   "source": [
    "Модели в OPR ожидают определенный формат входных данных:\n",
    "- `images_<camera_name>` - батч изображений для камеры `<camera_name>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe628bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frame = database_reader[id_to_show]\n",
    "sample_output = model(sample_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea98053",
   "metadata": {},
   "source": [
    "**❗ Внимание:** `model.forward` ожидает на вход **батч** из словаря с ключами `images_<camera_name>` - размерность батча должна быть `(B, 3, H, W)`, где `B` - количество изображений в батче, `H` и `W` - высота и ширина изображений соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "database_dl = torch.utils.data.DataLoader(\n",
    "    database_reader,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=database_reader.collate_fn,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "sample_batch = next(iter(database_dl))\n",
    "print(f\"Batch sample keys: {sample_batch.keys()}\")\n",
    "print(f\"Batch sample shapes: {[v.shape for v in sample_batch.values()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model(sample_batch)\n",
    "\n",
    "print(f\"Sample output keys: {sample_output.keys()}\")\n",
    "print(f\"Sample output shapes: {[v.shape for v in sample_output.values()]}\")\n",
    "print(f\"Sample descriptor shape: {sample_output['final_descriptor'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01ca21",
   "metadata": {},
   "source": [
    "## Подготовка БД"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72009d55",
   "metadata": {},
   "source": [
    "Для использования пайплайна инференса OPR необходимо подготовить базу данных в формате `faiss` индекса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c030b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "database_dl = torch.utils.data.DataLoader(\n",
    "    database_reader,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=database_reader.collate_fn,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir = DATA_DIR / \"test\" / \"database\"\n",
    "database_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "descriptors_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(database_dl):\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        descriptors = model(batch)[\"final_descriptor\"]\n",
    "        descriptors_list.append(descriptors)\n",
    "descriptors = torch.cat(descriptors_list, dim=0)\n",
    "print(f\"Descriptors shape: {descriptors.shape}\")\n",
    "\n",
    "# Create L2 distance FAISS index for nearest neighbor search\n",
    "faiss_index = faiss.IndexFlatL2(descriptors.shape[1])\n",
    "faiss_index.add(descriptors.cpu().numpy())\n",
    "faiss.write_index(\n",
    "    faiss_index,\n",
    "    str(database_dir / \"index.faiss\")\n",
    ")\n",
    "\n",
    "# Copy pose data as track.csv (required by PlaceRecognitionPipeline)\n",
    "shutil.copy(DATA_DIR / \"test\" / \"07_2023-10-04-day\" / \"track.csv\", database_dir / \"track.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4196a87",
   "metadata": {},
   "source": [
    "## Sequence-based baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a367e",
   "metadata": {},
   "source": [
    "В качестве бейзлайна для обработки последовательностей предлагается использовать алгоритм **Candidate Pool Fusion**:\n",
    "\n",
    "![candidate_pool_fusion](../images/candidate_pool_fusion.jpg)\n",
    "\n",
    "См. код в `opr.pipelines.place_recognition.sequential`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558ebbd",
   "metadata": {},
   "source": [
    "## Чтение query данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bfd0f0",
   "metadata": {},
   "source": [
    "Для удобства напишем небольшой Wrapper, который будет читать несколько фреймов из исходного DataReader и возвращать их в виде последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f05ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackSeqWrapper:\n",
    "    def __init__(self, track_data_reader: ITLPTrackDataReader, seq_len: int = 3):\n",
    "        \"\"\"Wrapper for ITLPTrackDataReader to provide sequences of specified length.\"\"\"\n",
    "        self.track_data_reader = track_data_reader\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.track_data_reader)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> list[dict[str, Tensor]]:\n",
    "        \"\"\"Get a sequence of frames up to the given index.\"\"\"\n",
    "        sequence = []\n",
    "        for i in range(max(0, idx - self.seq_len + 1), idx + 1):\n",
    "            sequence.append(self.track_data_reader[i])\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b96773",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_reader = ITLPTrackDataReader(\n",
    "    root=DATA_DIR / \"test\" / \"08_2023-10-11-night\",\n",
    "    image_transform=setup_transforms(image_size=322),\n",
    ")\n",
    "\n",
    "seq_data_reader = TrackSeqWrapper(\n",
    "    track_data_reader=query_reader,\n",
    "    seq_len=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141efdaf",
   "metadata": {},
   "source": [
    "## Инференс"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a84b884",
   "metadata": {},
   "source": [
    "См. реализации в коде библиотеки OPR:\n",
    "- `opr.models.place_recognition.sequential.SequenceLateFusionModel`\n",
    "- `opr.modules.temporal.TemporalAveragePooling`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee79a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = SequenceLateFusionModel(\n",
    "    model=model,\n",
    "    temporal_fusion_module=TemporalAveragePooling()\n",
    ")\n",
    "\n",
    "pipe = SequencePlaceRecognitionPipeline(\n",
    "    database_dir=DATA_DIR / \"test\" / \"database\",\n",
    "    model=seq_model,\n",
    "    use_candidate_pool_fusion=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225157ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = []\n",
    "for query_seq in tqdm(seq_data_reader):\n",
    "    output = pipe.infer(query_seq)\n",
    "    output_ids.append(output['idx'])\n",
    "\n",
    "with open(SUBMISSIONS_DIR / \"baseline.txt\", \"w\") as f:\n",
    "    for idx in output_ids:\n",
    "        f.write(f\"{idx}\\n\")\n",
    "\n",
    "print(f\"Submissions saved to {SUBMISSIONS_DIR / 'baseline.txt'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iprofihack2025-phystech-cartography",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
