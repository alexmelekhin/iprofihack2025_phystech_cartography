{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68006715",
   "metadata": {},
   "source": [
    "# Как посчитать метрику Recall@1 самому"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c41ea",
   "metadata": {},
   "source": [
    "**Внимание:** это не самое \"вычислительно эффективное\" решение, но оно позволяет понять, как работает метрика Recall@1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14db250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "import torchshow as ts\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import opr\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from opr.models.place_recognition import BoQModel, SequenceLateFusionModel\n",
    "from opr.modules.temporal import TemporalAveragePooling\n",
    "from opr.pipelines.place_recognition.sequential import SequencePlaceRecognitionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a908cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ROOT = Path.cwd().parent\n",
    "print(f\"Repository root dir: {REPO_ROOT}\")\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "assert DATA_DIR.exists(), f\"Data directory {DATA_DIR} does not exist. Please run the download script.\"\n",
    "print(f\"Data dir: {DATA_DIR}\")\n",
    "\n",
    "SUBMISSIONS_DIR = REPO_ROOT / \"submissions\"\n",
    "SUBMISSIONS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"Submissions dir: {SUBMISSIONS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_TRACK_DIR = DATA_DIR / \"train-val\" / \"00_2023-02-10-twilight\"\n",
    "QUERY_TRACK_DIR = DATA_DIR / \"train-val\" / \"01_2023-02-21-day\"\n",
    "\n",
    "print(f\"Database track dir: {DATABASE_TRACK_DIR}\")\n",
    "print(f\"Query track dir: {QUERY_TRACK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ffccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITLPTrackDataReader:\n",
    "    def __init__(self, root: Path, image_transform: A.Compose, front_cam: bool = True, back_cam: bool = False):\n",
    "        self._root = Path(root)\n",
    "        self._front_cam_dir = self._root / \"front_cam\"\n",
    "        self._back_cam_dir = self._root / \"back_cam\"\n",
    "\n",
    "        self._track_df = pd.read_csv(self._root / \"track.csv\")\n",
    "        self._image_transform = image_transform  # note that we use albumentations for image transformations\n",
    "        self._front_cam = front_cam\n",
    "        self._back_cam = back_cam\n",
    "        if not self._front_cam and not self._back_cam:\n",
    "            raise ValueError(\"At least one camera must be enabled: front_cam or back_cam.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._track_df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict[str, Tensor]:\n",
    "        pose = self._track_df[[\"tx\", \"ty\"]].iloc[idx].to_numpy()\n",
    "        front_cam_path = self._front_cam_dir / f\"{self._track_df['front_cam_ts'].iloc[idx]}.jpg\"\n",
    "        back_cam_path = self._back_cam_dir / f\"{self._track_df['back_cam_ts'].iloc[idx]}.jpg\"\n",
    "\n",
    "        out_dict = {\"pose\": Tensor(pose)}\n",
    "\n",
    "        if self._front_cam:\n",
    "            front_cam_image = cv2.cvtColor(cv2.imread(str(front_cam_path)), cv2.COLOR_BGR2RGB)\n",
    "            front_cam_image = self._image_transform(image=front_cam_image)[\"image\"]  #\n",
    "            out_dict[\"image_front_cam\"] = front_cam_image\n",
    "        if self._back_cam:\n",
    "            back_cam_image = cv2.cvtColor(cv2.imread(str(back_cam_path)), cv2.COLOR_BGR2RGB)\n",
    "            back_cam_image = self._image_transform(image=back_cam_image)[\"image\"]\n",
    "            out_dict[\"image_back_cam\"] = back_cam_image\n",
    "\n",
    "        return out_dict\n",
    "\n",
    "    def collate_fn(self, batch: list[dict[str, Tensor]]) -> dict[str, Tensor]:\n",
    "        collated_batch = {}\n",
    "        for key in batch[0].keys():\n",
    "            if key.startswith(\"image_\"):\n",
    "                collated_batch[\"images_\" + key[6:]] = torch.stack([item[key] for item in batch])\n",
    "            elif key == \"pose\":\n",
    "                collated_batch[\"poses\"] = torch.stack([item[key] for item in batch])\n",
    "        return collated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_transforms(image_size: int = 322) -> A.Compose:  # 384 for ResNet50, 322 for DINOv2\n",
    "    \"\"\"Create image transformation pipeline.\"\"\"\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.CenterCrop(height=720, width=720),  # Crop to 720x720 for 1:1 aspect ratio\n",
    "            A.Resize(height=image_size, width=image_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            A.pytorch.ToTensorV2(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a471a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_reader = ITLPTrackDataReader(\n",
    "    root=DATABASE_TRACK_DIR,\n",
    "    image_transform=setup_transforms(image_size=322)  # Use 384 for ResNet50, 322 for DINOv2\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "database_dl = torch.utils.data.DataLoader(\n",
    "    database_reader,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=database_reader.collate_fn,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f263c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BoQModel(backbone_name=\"dinov2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765704da",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_for_pipe_dir = DATA_DIR / \"database\"\n",
    "database_for_pipe_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "descriptors_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(database_dl):\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        descriptors = model(batch)[\"final_descriptor\"]\n",
    "        descriptors_list.append(descriptors)\n",
    "descriptors = torch.cat(descriptors_list, dim=0)\n",
    "print(f\"Descriptors shape: {descriptors.shape}\")\n",
    "\n",
    "# Create L2 distance FAISS index for nearest neighbor search\n",
    "faiss_index = faiss.IndexFlatL2(descriptors.shape[1])\n",
    "faiss_index.add(descriptors.cpu().numpy())\n",
    "faiss.write_index(\n",
    "    faiss_index,\n",
    "    str(database_for_pipe_dir / \"index.faiss\")\n",
    ")\n",
    "\n",
    "# Copy pose data as track.csv (required by PlaceRecognitionPipeline)\n",
    "shutil.copy(DATABASE_TRACK_DIR/ \"track.csv\", database_for_pipe_dir / \"track.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4beec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackSeqWrapper:\n",
    "    def __init__(self, track_data_reader: ITLPTrackDataReader, seq_len: int = 3):\n",
    "        \"\"\"Wrapper for ITLPTrackDataReader to provide sequences of specified length.\"\"\"\n",
    "        self.track_data_reader = track_data_reader\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.track_data_reader)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> list[dict[str, Tensor]]:\n",
    "        \"\"\"Get a sequence of frames up to the given index.\"\"\"\n",
    "        sequence = []\n",
    "        for i in range(max(0, idx - self.seq_len + 1), idx + 1):\n",
    "            sequence.append(self.track_data_reader[i])\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d02214",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_reader = ITLPTrackDataReader(\n",
    "    root=QUERY_TRACK_DIR,\n",
    "    image_transform=setup_transforms(image_size=322),\n",
    ")\n",
    "\n",
    "seq_data_reader = TrackSeqWrapper(\n",
    "    track_data_reader=query_reader,\n",
    "    seq_len=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d976d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = SequenceLateFusionModel(\n",
    "    model=model,\n",
    "    temporal_fusion_module=TemporalAveragePooling()\n",
    ")\n",
    "\n",
    "pipe = SequencePlaceRecognitionPipeline(\n",
    "    database_dir=database_for_pipe_dir,\n",
    "    model=seq_model,\n",
    "    use_candidate_pool_fusion=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cf814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPPER_LIMIT = np.inf\n",
    "UPPER_LIMIT = 50  # SET TO SOME SMALL VALUE FOR DEBUGGING / SET TO np.inf FOR FULL RUN\n",
    "\n",
    "output_ids = []\n",
    "for i, query_seq in tqdm(enumerate(seq_data_reader)):\n",
    "    if i >= UPPER_LIMIT:\n",
    "        break  # Limit the number of sequences for testing\n",
    "    output = pipe.infer(query_seq)\n",
    "    output_ids.append(output['idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653deb3",
   "metadata": {},
   "source": [
    "## Расчет метрики Recall@1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f73f06",
   "metadata": {},
   "source": [
    "(примерно такой же код под капотом у чекера на Яндекс.Контесте)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85065c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_THRESHOLD = 10.0\n",
    "\n",
    "query_df = pd.read_csv(QUERY_TRACK_DIR / \"track.csv\")\n",
    "database_df = pd.read_csv(DATABASE_TRACK_DIR / \"track.csv\")\n",
    "database_coords = database_df[[\"tx\", \"ty\"]].values\n",
    "\n",
    "gt_lines = []\n",
    "\n",
    "for _, row in query_df.iterrows():\n",
    "    coords = row[[\"tx\", \"ty\"]].values\n",
    "    distances = ((database_coords - coords) ** 2).sum(axis=1) ** 0.5\n",
    "    match_indices = np.argwhere(distances < DIST_THRESHOLD).flatten()\n",
    "    if match_indices.size == 0:\n",
    "        gt_lines.append(\"-1\\n\")\n",
    "        continue\n",
    "    indices_str = \" \".join(map(str, match_indices))\n",
    "    gt_lines.append(f\"{indices_str}\\n\")\n",
    "\n",
    "\n",
    "matched = []\n",
    "for a, gt in zip(output_ids, gt_lines[:UPPER_LIMIT]):\n",
    "    if gt == \"-1\\n\":  # -1 means that there is no true answer and we should simply skip it\n",
    "        continue\n",
    "    if str(a) in gt:\n",
    "        matched.append(1)\n",
    "    else:\n",
    "        matched.append(0)\n",
    "r_at_1 = sum(matched) / len(matched) if matched else 0\n",
    "\n",
    "print(r_at_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2787b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iprofihack2025-phystech-cartography",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
